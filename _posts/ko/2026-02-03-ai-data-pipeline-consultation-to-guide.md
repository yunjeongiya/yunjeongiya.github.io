---
layout: post
title: "3000개 상담에서 33개 학습 가이드 만들기 - AI 데이터 파이프라인"
published: false
date: 2026-02-03 19:00:00 +0900
categories: [Development, AI]
tags: [openai, embedding, clustering, dbscan, gpt, data-pipeline]
lang: ko
slug: "028"
thumbnail: /assets/images/posts/028-ai-data-pipeline/thumbnail-ko.png
---

> 이전 글: [로컬 캐시의 함정 - Notion 3000개 페이지 본문 추출 삽질기](/posts/027-notion-local-cache-limit/)

## TL;DR

- 3,094개 상담일지에서 **25,782개 학습 조언**을 추출했다
- 임베딩 + DBSCAN으로 **570개 클러스터**로 분류했다
- GPT로 클러스터를 합성해 **33개 체계적인 학습 가이드**를 만들었다
- 수동으로 분류했으면 몇 주 걸렸을 작업을 **하루 만에** 완료

---

## 배경: 상담 데이터의 보물

[지난번에 Notion에서 3,094개 상담일지를 추출했다](/posts/027-notion-local-cache-limit/). 수년간 쌓인 학생 상담 기록. 그 안에는 수학 공부법, 영단어 암기법, 오답 분석 방법 등 **검증된 학습 노하우**가 묻혀있었다.

문제는 양이다. 3,094개 상담을 사람이 읽고 정리하려면 몇 주가 걸린다.

**목표**: AI로 자동 분류하고, 체계적인 학습 가이드로 정리하기.

---

## 파이프라인 개요

```
┌─────────────────────────────────────────────────────┐
│ 1. 상담일지 3,094개                                   │
│    "영단어는 짧게 여러 번 보는 게 효과적이에요"          │
│    "오답노트 쓸 때 왜 틀렸는지 적어야 해요"             │
└─────────────────────────────────────────────────────┘
                          ↓ GPT 추출
┌─────────────────────────────────────────────────────┐
│ 2. 학습 조언 25,782개 (원문 인용)                      │
│    "밥 먹고 10분, 자습 시작 전 10분 확보하기"           │
│    "선지마다 근거 찾기, 지문에 표시하기"                │
└─────────────────────────────────────────────────────┘
                          ↓ 임베딩 + 클러스터링
┌─────────────────────────────────────────────────────┐
│ 3. 클러스터 570개                                     │
│    클러스터 #42: 영단어 암기 (1,377개 인용)            │
│    클러스터 #15: 국어 오답 분석 (970개 인용)           │
└─────────────────────────────────────────────────────┘
                          ↓ GPT 합성
┌─────────────────────────────────────────────────────┐
│ 4. 학습 가이드 33개                                   │
│    "[영어] 영어 단어 암기 방법" (체계적인 마크다운)      │
│    "[국어] 국어 오답 분석 방법" (단계별 가이드)         │
└─────────────────────────────────────────────────────┘
```

---

## Step 1: 조언 추출 (요약 vs 인용)

### "추출 없이 바로 클러스터링하면 안 되나?"

처음에는 이 단계가 토큰 낭비라고 생각했다. 상담 원문을 그대로 임베딩하면 되지 않나?

안 된다. 상담일지에는 학습 조언만 있는 게 아니다:

```
"안녕하세요~ 오늘 기분 어때요?"          ← 인사
"지난주에 약속한 건 잘 했어요?"          ← 점검
"영단어는 짧게 여러 번 보는 게 좋아요"    ← 학습 조언 ✅
"다음 주에도 같이 해봐요, 화이팅!"       ← 마무리
```

이걸 그대로 클러스터링하면? "안녕하세요" 1,500개가 하나의 클러스터, "화이팅" 800개가 또 하나의 클러스터. **인사 클러스터, 마무리 클러스터**가 상위권을 차지하고, 정작 학습 조언은 묻힌다.

GPT로 먼저 **학습 조언만 필터링**해야 의미 있는 클러스터가 나온다.

### 첫 시도: GPT에게 요약 시키기

```python
prompt = """
이 상담에서 학습 방법을 요약해주세요.
"""
```

결과:

```
"효과적인 암기를 위해 반복 학습이 중요합니다."
```

**문제**: 너무 추상적이다. "반복 학습"이 구체적으로 뭔데?

### 해결: 원문 그대로 인용

```python
prompt = """
이 상담에서 구체적인 학습 조언을 **원문 그대로** 인용해주세요.
요약하지 마세요!

예시:
- "밥 먹고 10분, 자습 시작 전 10분"
- "선지마다 근거 찾기, 틀린 이유 → 어떻게 해야 맞았을까까지"
"""
```

결과:

```json
{
  "quotes": [
    {
      "text": "1. 쭉 보면서 모르는 거 체크 → 2. 다시 체크한 것만 보기",
      "type": "단계별 학습 방법",
      "subject": "공통"
    },
    {
      "text": "답답함만 느끼면 큰일납니다. 답답하면 질문하세요.",
      "type": "마인드셋",
      "subject": "공통"
    }
  ]
}
```

**구체적인 문장**이 살아있다. 이게 진짜 노하우다.

### 추출 결과

| 항목 | 수치 |
|------|------|
| 입력 상담 | 3,094개 |
| 추출된 인용 | 25,782개 |
| 상담당 평균 | 8.3개 |
| API 비용 | ~$15 (GPT-4o-mini) |

---

## Step 2: 임베딩 + 클러스터링

25,782개 인용을 어떻게 분류할까? 키워드 매칭? "영단어"가 들어가면 영어 카테고리?

```
"영단어는 짧게 여러 번"     → 영어/암기
"수학 공식도 짧게 여러 번"   → 수학/암기 (같은 방법인데?)
```

**같은 학습법**인데 과목이 달라서 분리된다.

### 해결: 의미 기반 클러스터링

```python
from sklearn.cluster import DBSCAN
import numpy as np

# 1. 텍스트 → 벡터 (OpenAI Embedding)
embeddings = embed_texts(quotes)  # 25,782 x 1536 차원

# 2. 과목별로 먼저 분리
for subject in ['국어', '영어', '수학', '탐구', '공통']:
    subject_quotes = filter_by_subject(quotes, subject)
    subject_embeddings = get_embeddings(subject_quotes)

    # 3. DBSCAN 클러스터링
    clustering = DBSCAN(
        eps=0.25,           # 거리 임계값 (낮을수록 촘촘)
        min_samples=3,      # 최소 클러스터 크기
        metric='cosine'     # 코사인 유사도
    )
    labels = clustering.fit_predict(subject_embeddings)
```

### DBSCAN을 선택한 이유

| 알고리즘 | 장점 | 단점 |
|---------|------|------|
| K-Means | 빠름 | K를 미리 정해야 함 |
| **DBSCAN** | K 불필요, 노이즈 처리 | eps 튜닝 필요 |
| Hierarchical | 계층 구조 | 느림 |

25,782개 인용이 몇 개 클러스터가 될지 몰랐다. DBSCAN은 **자동으로 클러스터 수를 결정**한다.

### 클러스터링 결과

| 항목 | 수치 |
|------|------|
| 입력 인용 | 25,782개 |
| 생성된 클러스터 | 570개 |
| 노이즈 (아웃라이어) | 2,341개 |
| 가장 큰 클러스터 | 1,377개 (영단어 암기) |
| 임베딩 비용 | ~$3 (text-embedding-3-small) |

---

## Step 3: 아티클 합성

클러스터 하나에 수백 개의 비슷한 인용이 있다. 이걸 그대로 보여주면?

```markdown
## 영단어 암기 방법

- 짧게 여러 번 보기
- 하루에 여러 번 짧게 보기
- 한 번에 오래 보지 말고 여러 번
- 반복이 중요
- 자주 보는 게 좋음
... (1,377개)
```

**중복 투성이**다. 읽을 수 없다.

### 해결: GPT로 합성

```python
prompt = f"""
다음은 "{title}" 주제에 대해 여러 상담에서 추출한 조언들입니다.

## 요구사항:
1. 중복 제거: 비슷한 내용은 하나로 통합
2. 체계적 구성: 단계별 또는 상황별로 정리
3. 실용적 조언: 구체적이고 실행 가능한 팁 중심
4. 핵심 보존: 원본의 좋은 아이디어는 모두 포함

## 원본 인용들 ({len(quotes)}개):
{quotes_text}

위 내용을 종합한 학습 가이드를 작성해주세요.
"""
```

### 합성 결과 예시

**Before (1,377개 인용):**
```
- 짧게 여러 번 보기
- 밥 먹고 10분 단어 보기
- 아침에 보고 점심에 또 보기
- 저녁에 테스트하기
- 모르는 것만 체크하기
... (1,372개 더)
```

**After (하나의 가이드):**

```markdown
## 핵심 원칙

영어 단어 암기의 핵심은 **한 번에 오래 보는 것이 아니라,
짧게 여러 번 반복**하는 것이다.

## 일일 단어 암기 루틴

### 1단계: 아침 - 필터링
1. 단어장을 펴고 뜻을 가린다
2. 확실히 아는 단어는 지운다
3. 남은 단어만 2번 정독

### 2단계: 점심 전 - 1차 복습
- 아까 본 단어 다시 2번 정독
- 여전히 모르는 건 체크 표시

### 3단계: 점심 후 - 테스트
- 뜻을 가리고 시험
- 틀린 것에 체크
...
```

**1,377개 → 1개** 체계적인 가이드로 압축됐다.

---

## Step 4: 품질 다듬기

GPT가 생성한 33개 아티클에 일관성 문제가 있었다.

### 문제 1: 문체 혼용

```
# 어떤 아티클
"복습하세요", "중요합니다"  (합니다체)

# 다른 아티클
"복습하라", "중요하다"  (하다체)
```

### 해결: 일괄 변환

```python
replacements = [
    ('합니다', '한다'),
    ('됩니다', '된다'),
    ('하세요', '하라'),
    ('보세요', '봐라'),
    ...
]

for old, new in replacements:
    content = content.replace(old, new)
```

### 문제 2: 오타

GPT가 만든 어색한 표현들:

```
"충분한다" → "충분하다"
"해라." → "하라."
```

### 최종 수정

| 항목 | 수정 건수 |
|------|----------|
| 문체 통일 | 81개 |
| 오타 수정 | 50개 |
| 도입부 제거 | 13개 |

---

## 결과물

### 33개 학습 가이드

| 과목 | 아티클 수 | 예시 |
|------|----------|------|
| 공통 | 8개 | 시간 관리, 계획 수립, 오답 분석 |
| 국어 | 6개 | 문학 분석, 독서 분석, 언매화작 |
| 수학 | 5개 | 문제 풀이, 개념 학습, 킬러 문항 |
| 영어 | 4개 | 단어 암기, 독해, 문법 |
| 탐구 | 10개 | 생명과학, 물리, 화학, 지구과학 등 |

### 개조식 / 설명식 토글

같은 내용을 두 가지 스타일로 제공:

**개조식** (빠른 참조용):
```
## 1단계: 필터링
- 뜻 가리고 테스트
- 아는 단어 지우기
- 모르는 것만 남기기
```

**설명식** (자세한 이해용):
```
## 1단계: 필터링

하루를 시작할 때 단어장을 펴고 뜻을 가린다.
이미 확실히 아는 단어는 과감하게 지운다.
love처럼 누구나 아는 단어에 시간을 쓸 필요가 없다.
```

---

## 비용 & 시간

| 단계 | 도구 | 비용 | 시간 |
|------|------|------|------|
| 데이터 추출 | Notion API | 무료 | 40분 |
| 조언 추출 | GPT-4o-mini | ~$15 | 2시간 |
| 임베딩 | text-embedding-3-small | ~$3 | 30분 |
| 클러스터링 | scikit-learn | 무료 | 5분 |
| 아티클 합성 | GPT-4o-mini | ~$5 | 1시간 |
| 품질 다듬기 | Python 스크립트 | 무료 | 30분 |
| **합계** | | **~$23** | **~5시간** |

수동으로 했다면? **몇 주**.

---

## 교훈

### 1. 요약보다 인용

GPT에게 "요약해줘"라고 하면 추상적인 결과가 나온다. "원문 그대로 인용해줘"라고 하면 구체적인 노하우가 살아남는다.

### 2. 의미 기반 분류

키워드 매칭은 한계가 있다. 임베딩 + 클러스터링으로 **의미적으로 비슷한 것**을 묶어야 한다.

### 3. 합성의 힘

1,000개의 비슷한 문장을 보여주는 것보다, GPT가 합성한 하나의 체계적인 가이드가 훨씬 유용하다.

### 4. 마무리는 사람이

GPT가 만든 결과물도 문체 혼용, 오타 같은 문제가 있다. 최종 검수는 필요하다.

---

## 다음 단계

- [ ] CheckUS 솔루션 백과사전(F127)에 통합
- [ ] 상담 중 자동 추천 (RAG)
- [ ] 신입 선생님 교육 자료로 활용

---
